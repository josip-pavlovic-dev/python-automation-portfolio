# DAY 01: Python Automation Kickoff ğŸš€

**Datum:** 13. decembar 2025.
**Fokus:** Python Refresh + Web Scraper Projekt -START
**Vreme:** 8-10 sati intenzivnog rada
**Energija:** SVE ILI NIÅ TA! ğŸ’ª

---

## ğŸ¯ Cilj Dana

**Do kraja danas:**

1. âœ… Repo kreiran i inicijalizovan
2. âœ… Python okruÅ¾enje konfigurisano
3. âœ… Prvi scraper kod napisan (basic verzija)
4. âœ… Prvo testiranje u terminalu - radi!
5. âœ… Prvi commit na GitHub
6. âœ… Python ritam uspostavljen - seÄ‡aÅ¡ se sintakse!

**Ovo NIJE teorija.** Danas ODMAH kodiÅ¡. Ako negde zaglaviÅ¡, AI je tu u VS Code-u!

---

## ğŸ“‹ Struktura Dana (10h verzija)

### BLOK 1: Setup + Python Refresh (09:00-11:00) - 2h

**Zadatak 1.1: Python Environment Setup (30min)**

```bash
# Proveri Python verziju
python3 --version  # OÄekujem 3.10+

# Kreiraj virtual environment za projekat
cd ~/code/python-automation-lab/python-automation-portfolio/projects/01-web-scraper
python3 -m venv venv

# Aktiviraj okruÅ¾enje
source venv/bin/activate

# Upgrade pip
pip install --upgrade pip
```

**Zadatak 1.2: Instaliraj Dependencies (10min)**

```bash
# Osnovne biblioteke za web scraping
pip install requests beautifulsoup4 pandas lxml

# Snimi requirements
pip freeze > requirements.txt
```

**Zadatak 1.3: Python Syntax Refresh - Hands-On! (1h 20min)**

Otvori novi fajl: `python_refresh.py`

```python
# 1. VARIABLES & TYPES (10min)
# =========================
name = "Jole"
age = 30  # pretpostavljam :)
is_motivated = True
skills = ["Python", "automation", "problem-solving"]

print(f"Ime: {name}, Godina: {age}")
print(f"Skills: {', '.join(skills)}")

# 2. FUNCTIONS (15min)
# ====================
def greet(name):
    """Pozdravlja korisnika."""
    return f"Hello, {name}! Ready to scrape! ğŸ•·ï¸"

def calculate_discount(price, discount_percent):
    """RaÄuna cenu sa popustom."""
    discount = price * (discount_percent / 100)
    return price - discount

# Testiranje
print(greet("Jole"))
print(f"Cena sa popustom: {calculate_discount(100, 20)} EUR")

# 3. LISTS & LOOPS (20min)
# =========================
urls = [
    "https://example.com/page1",
    "https://example.com/page2",
    "https://example.com/page3"
]

# For petlja
for url in urls:
    print(f"Scraping: {url}")

# List comprehension (Python naÄin!)
urls_upper = [url.upper() for url in urls]
print(urls_upper)

# 4. DICTIONARIES (20min)
# =======================
product = {
    "name": "Laptop",
    "price": 1200,
    "brand": "Dell",
    "in_stock": True
}

# Pristup vrednostima
print(f"Proizvod: {product['name']}, Cena: {product['price']} EUR")

# Iteracija kroz dictionary
for key, value in product.items():
    print(f"{key}: {value}")

# Lista dictionaries (tipiÄan scraping output!)
products = [
    {"name": "Laptop", "price": 1200},
    {"name": "Mouse", "price": 25},
    {"name": "Keyboard", "price": 75}
]

for product in products:
    print(f"{product['name']}: â‚¬{product['price']}")

# 5. FILE OPERATIONS (15min)
# ===========================
# Pisanje u fajl
with open("test_output.txt", "w") as f:
    f.write("Hello from Python!\n")
    f.write("Web scraping is awesome!\n")

# ÄŒitanje iz fajla
with open("test_output.txt", "r") as f:
    content = f.read()
    print(content)

# CSV operacije (biÄ‡e ti jako potrebno!)
import csv

data = [
    ["Name", "Price", "Brand"],
    ["Laptop", "1200", "Dell"],
    ["Mouse", "25", "Logitech"]
]

with open("test_products.csv", "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerows(data)

print("CSV fajl kreiran!")
```

**ZADATAK:** Otvori `python_refresh.py`, pokreni kod sekciju po sekciju. Promeni vrednosti, eksperimentuÅ¡i!

```bash
python python_refresh.py
```

---

### BLOK 2: Web Scraping Basics (11:15-13:15) - 2h

**Zadatak 2.1: Requests Biblioteka (30min)**

Kreiraj novi fajl: `test_requests.py`

```python
import requests

# 1. Osnovni GET request
url = "https://httpbin.org/get"
response = requests.get(url)

print(f"Status Code: {response.status_code}")
print(f"Content Type: {response.headers['Content-Type']}")
print(f"Response Text:\n{response.text[:200]}...")  # Prva 200 karaktera

# 2. Provera da li je request uspeÅ¡an
if response.status_code == 200:
    print("âœ… Request successful!")
else:
    print(f"âŒ Request failed with code {response.status_code}")

# 3. JSON response
url_json = "https://jsonplaceholder.typicode.com/users/1"
response_json = requests.get(url_json)
data = response_json.json()  # Parsiraj JSON

print(f"\nUser Name: {data['name']}")
print(f"Email: {data['email']}")
print(f"City: {data['address']['city']}")

# 4. Custom headers (vaÅ¾no za scraping!)
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}
response_with_headers = requests.get("https://httpbin.org/headers", headers=headers)
print(f"\nResponse with custom headers:\n{response_with_headers.text[:300]}...")
```

**Pokreni i vidi kako funkcioniÅ¡e:**

```bash
python test_requests.py
```

**Zadatak 2.2: BeautifulSoup Parsing (1h)**

Kreiraj: `test_beautifulsoup.py`

```python
import requests
from bs4 import BeautifulSoup

# 1. UÄitaj HTML stranicu
url = "https://quotes.toscrape.com/"  # OdliÄan sajt za veÅ¾banje!
response = requests.get(url)
html = response.text

# 2. Parsiraj HTML sa BeautifulSoup
soup = BeautifulSoup(html, 'lxml')

# 3. Izvuci naslov stranice
title = soup.find('title')
print(f"Page Title: {title.text}")

# 4. NaÄ‘i sve quote tekstove
quotes = soup.find_all('span', class_='text')
print(f"\nğŸ“– Ukupno citata: {len(quotes)}\n")

for i, quote in enumerate(quotes[:5], 1):  # Prvih 5
    print(f"{i}. {quote.text}")

# 5. Izvuci autore
authors = soup.find_all('small', class_='author')
print(f"\nâœï¸ Autori:\n")

for i, author in enumerate(authors[:5], 1):
    print(f"{i}. {author.text}")

# 6. Struktuirani podaci - PRAVI NAÄŒIN!
quotes_data = []

# Svaki quote je u <div class="quote">
quote_divs = soup.find_all('div', class_='quote')

for quote_div in quote_divs:
    # Izvuci text, author, i tags
    text = quote_div.find('span', class_='text').text
    author = quote_div.find('small', class_='author').text
    tags = [tag.text for tag in quote_div.find_all('a', class_='tag')]

    quotes_data.append({
        'text': text,
        'author': author,
        'tags': ', '.join(tags)
    })

# PrikaÅ¾i strukturirane podatke
print("\nğŸ“Š Strukturirani podaci:\n")
for i, quote in enumerate(quotes_data[:3], 1):
    print(f"{i}.")
    print(f"   Text: {quote['text']}")
    print(f"   Author: {quote['author']}")
    print(f"   Tags: {quote['tags']}\n")

# 7. Export u CSV koristeÄ‡i pandas
import pandas as pd

df = pd.DataFrame(quotes_data)
df.to_csv('quotes_scraped.csv', index=False, encoding='utf-8')
print("âœ… CSV fajl kreiran: quotes_scraped.csv")
```

**Pokreni:**

```bash
python test_beautifulsoup.py
```

**PROVERITE CSV:**

```bash
cat quotes_scraped.csv  # Ili otvori u Excel-u!
```

**Zadatak 2.3: CSS Selectors Deep Dive (30min)**

Dodaj na kraj `test_beautifulsoup.py`:

```python
# CSS SELECTORS - moÄ‡niji naÄin!
print("\nğŸ¯ CSS SELECTORS:\n")

# 1. Selektuj po klasi
quotes_css = soup.select('span.text')
print(f"Quotes via CSS selector: {len(quotes_css)}")

# 2. Selektuj po ID (ako postoji)
# container = soup.select_one('#content')  # Primer

# 3. Nested selectors
authors_css = soup.select('div.quote small.author')
print(f"Authors via nested selector: {len(authors_css)}")

# 4. Kombinovani selektori
tags_css = soup.select('div.quote a.tag')
print(f"Tags via selector: {len(tags_css)}")

# 5. Atributi
links = soup.select('a[href]')  # Svi linkovi sa href
print(f"Total links: {len(links)}")
```

---

### ğŸ” PAUZA ZA RUÄŒAK (13:15-14:15) - 1h

**NE PRESKAÄŒI!** Mozak treba odmor. Jedi, proÅ¡etaj, resetuj se.

---

### BLOK 3: Prvi Pravi Scraper (14:15-17:15) - 3h

**SADA PRAVIÅ  PRAVI PROJEKAT!**

**Zadatak 3.1: Projekat Setup (15min)**

Organizuj foldere:

```bash
cd ~/code/python-automation-lab/python-automation-portfolio/projects/01-web-scraper

# Folder struktura:
# 01-web-scraper/
#   â”œâ”€â”€ scraper.py (glavni kod)
#   â”œâ”€â”€ config.py (konfiguracija)
#   â”œâ”€â”€ requirements.txt
#   â”œâ”€â”€ README.md
#   â”œâ”€â”€ tests/ (kasnije)
#   â””â”€â”€ output/ (gde se Äuva CSV)

mkdir -p output tests
```

**Zadatak 3.2: config.py - Best Practice! (10min)**

Kreiraj `config.py`:

```python
"""
Configuration file for web scraper.
"""

# Target websites
URLS = [
    "https://quotes.toscrape.com/page/1/",
    "https://quotes.toscrape.com/page/2/",
]

# Request headers (simuliraj browser)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# Output settings
OUTPUT_DIR = "output"
OUTPUT_FILE = "scraped_quotes.csv"

# Request settings
REQUEST_TIMEOUT = 10  # sekunde
DELAY_BETWEEN_REQUESTS = 1  # sekunda (budi pristojan prema serveru!)
```

**Zadatak 3.3: scraper.py - Glavni Kod! (2h 15min)**

Kreiraj `scraper.py`:

```python
"""
Web Scraper Tool - Quotes Scraper
Scrapes quotes from quotes.toscrape.com and saves to CSV.
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
from config import URLS, HEADERS, OUTPUT_DIR, OUTPUT_FILE, REQUEST_TIMEOUT, DELAY_BETWEEN_REQUESTS


def fetch_page(url):
    """
    Fetches HTML content from URL.

    Args:
        url (str): Target URL

    Returns:
        str: HTML content or None if failed
    """
    try:
        print(f"ğŸ“¥ Fetching: {url}")
        response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)

        if response.status_code == 200:
            print(f"âœ… Success! Status: {response.status_code}")
            return response.text
        else:
            print(f"âŒ Failed! Status: {response.status_code}")
            return None

    except requests.exceptions.RequestException as e:
        print(f"âŒ Error fetching {url}: {e}")
        return None


def parse_quotes(html):
    """
    Parses quotes from HTML content.

    Args:
        html (str): HTML content

    Returns:
        list: List of dictionaries with quote data
    """
    soup = BeautifulSoup(html, 'lxml')
    quotes_data = []

    quote_divs = soup.find_all('div', class_='quote')
    print(f"ğŸ” Found {len(quote_divs)} quotes on this page")

    for quote_div in quote_divs:
        try:
            text = quote_div.find('span', class_='text').text
            author = quote_div.find('small', class_='author').text
            tags = [tag.text for tag in quote_div.find_all('a', class_='tag')]

            quotes_data.append({
                'quote': text,
                'author': author,
                'tags': ', '.join(tags)
            })
        except AttributeError as e:
            print(f"âš ï¸ Warning: Could not parse quote - {e}")
            continue

    return quotes_data


def save_to_csv(data, output_path):
    """
    Saves scraped data to CSV file.

    Args:
        data (list): List of dictionaries
        output_path (str): Path to output CSV file
    """
    df = pd.DataFrame(data)
    df.to_csv(output_path, index=False, encoding='utf-8')
    print(f"ğŸ’¾ Saved {len(data)} quotes to {output_path}")


def main():
    """
    Main scraper logic.
    """
    print("ğŸš€ Starting Web Scraper...")
    print(f"ğŸ“‹ Target URLs: {len(URLS)}")
    print("=" * 60)

    all_quotes = []

    # Scrape svaki URL
    for i, url in enumerate(URLS, 1):
        print(f"\n[{i}/{len(URLS)}] Processing: {url}")

        html = fetch_page(url)

        if html:
            quotes = parse_quotes(html)
            all_quotes.extend(quotes)
            print(f"âœ… Extracted {len(quotes)} quotes")
        else:
            print(f"âš ï¸ Skipping {url} due to fetch error")

        # Delay izmeÄ‘u requesta (ne bombarduj server!)
        if i < len(URLS):  # Ne Äekaj posle poslednjeg
            print(f"â³ Waiting {DELAY_BETWEEN_REQUESTS}s before next request...")
            time.sleep(DELAY_BETWEEN_REQUESTS)

    # SaÄuvaj rezultate
    print("\n" + "=" * 60)
    print(f"ğŸ“Š Total quotes scraped: {len(all_quotes)}")

    if all_quotes:
        # Kreiraj output folder ako ne postoji
        os.makedirs(OUTPUT_DIR, exist_ok=True)

        output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILE)
        save_to_csv(all_quotes, output_path)

        print(f"âœ… Scraping complete! Check: {output_path}")
    else:
        print("âŒ No data scraped!")


if __name__ == "__main__":
    main()
```

**Zadatak 3.4: Testiranje! (20min)**

```bash
# Aktiviraj venv ako nije
source venv/bin/activate

# Pokreni scraper!
python scraper.py
```

**OÄekivani output:**

```
ğŸš€ Starting Web Scraper...
ğŸ“‹ Target URLs: 2
============================================================

[1/2] Processing: https://quotes.toscrape.com/page/1/
ğŸ“¥ Fetching: https://quotes.toscrape.com/page/1/
âœ… Success! Status: 200
ğŸ” Found 10 quotes on this page
âœ… Extracted 10 quotes
â³ Waiting 1s before next request...

[2/2] Processing: https://quotes.toscrape.com/page/2/
ğŸ“¥ Fetching: https://quotes.toscrape.com/page/2/
âœ… Success! Status: 200
ğŸ” Found 10 quotes on this page
âœ… Extracted 10 quotes

============================================================
ğŸ“Š Total quotes scraped: 20
ğŸ’¾ Saved 20 quotes to output/scraped_quotes.csv
âœ… Scraping complete! Check: output/scraped_quotes.csv
```

**PROVERI CSV:**

```bash
cat output/scraped_quotes.csv | head -n 5
# Ili otvori u Excel/LibreOffice!
```

---

### BLOK 4: README + Git Commit (17:30-19:00) - 1h 30min

**Zadatak 4.1: README.md za Projekat (45min)**

Kreiraj `projects/01-web-scraper/README.md`:

````markdown
# Web Scraper Tool

**Status:** âœ… MVP Complete (Dec 13, 2025)
**Tech Stack:** Python 3.10+, Requests, BeautifulSoup, Pandas

---

## ğŸ“– Description

Flexible web scraping tool that extracts structured data from websites and exports to CSV. Built for freelance clients needing data collection automation.

**Current Implementation:** Quotes scraper (demo)
**Customizable for:** Product listings, job postings, real estate data, competitor analysis

---

## ğŸš€ Features

-   âœ… Multiple URL scraping
-   âœ… Configurable headers (anti-blocking)
-   âœ… Rate limiting (respectful scraping)
-   âœ… Error handling & logging
-   âœ… CSV export with Pandas
-   âœ… Clean, documented code

---

## ğŸ› ï¸ Installation

```bash
# Clone repo
git clone <your-repo-url>
cd python-automation-portfolio/projects/01-web-scraper

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```
````

---

## ğŸ“¦ Dependencies

```
requests==2.31.0
beautifulsoup4==4.12.2
pandas==2.1.4
lxml==5.0.0
```

---

## ğŸ’» Usage

### Basic Usage

```bash
python scraper.py
```

Output saved to `output/scraped_quotes.csv`.

### Customize Target URLs

Edit `config.py`:

```python
URLS = [
    "https://your-target-site.com/page1",
    "https://your-target-site.com/page2",
]
```

### Adjust Settings

```python
# config.py
REQUEST_TIMEOUT = 10  # seconds
DELAY_BETWEEN_REQUESTS = 1  # seconds (be nice to servers!)
OUTPUT_FILE = "your_output.csv"
```

---

## ğŸ“‚ Project Structure

```
01-web-scraper/
â”œâ”€â”€ scraper.py          # Main scraper logic
â”œâ”€â”€ config.py           # Configuration settings
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ output/             # Scraped data (CSV files)
â”œâ”€â”€ tests/              # Unit tests (coming soon)
â””â”€â”€ venv/               # Virtual environment (not committed)
```

---

## ğŸ§ª Example Output

**CSV Format:**

```csv
quote,author,tags
"The world as we have created it...",Albert Einstein,"change,deep-thoughts,thinking,world"
"It is our choices...",J.K. Rowling,"abilities,choices"
```

---

## ğŸ”§ Customization for Clients

This scraper can be adapted for various use cases:

### E-commerce Price Monitoring

```python
# Parse product listings
def parse_products(html):
    soup = BeautifulSoup(html, 'lxml')
    products = []

    for item in soup.find_all('div', class_='product'):
        name = item.find('h2', class_='title').text
        price = item.find('span', class_='price').text
        products.append({'name': name, 'price': price})

    return products
```

### Job Listings Aggregation

```python
# Parse job postings
def parse_jobs(html):
    soup = BeautifulSoup(html, 'lxml')
    jobs = []

    for job in soup.find_all('div', class_='job-listing'):
        title = job.find('h3').text
        company = job.find('span', class_='company').text
        location = job.find('span', class_='location').text
        jobs.append({'title': title, 'company': company, 'location': location})

    return jobs
```

---

## âš ï¸ Legal & Ethical Considerations

-   Always check `robots.txt` before scraping
-   Respect rate limits (don't overload servers)
-   Some websites prohibit scraping in Terms of Service
-   Use scraped data responsibly and legally

**This tool is for educational and legitimate business purposes only.**

---

## ğŸ› Troubleshooting

### Issue: "Connection timeout"

**Solution:** Increase `REQUEST_TIMEOUT` in config.py

### Issue: "Status 403 Forbidden"

**Solution:** Update `User-Agent` header in config.py

### Issue: "No data found"

**Solution:** Inspect HTML with DevTools, adjust CSS selectors

---

## ğŸš€ Future Enhancements

-   [ ] Command-line arguments (argparse)
-   [ ] Selenium support for JavaScript-heavy sites
-   [ ] Proxy rotation
-   [ ] Database export (SQLite/PostgreSQL)
-   [ ] Scheduled scraping (cron jobs)
-   [ ] Email notifications on completion
-   [ ] Unit tests with pytest

---

## ğŸ“ Contact

For custom scraping projects:

**Jole PavloviÄ‡**
GitHub: [jole-pavlovic-dev]
Available for freelance work starting January 2026

---

**License:** MIT
**Last Updated:** December 13, 2025

````

**Zadatak 4.2: Git Commit! (30min)**

```bash
# Proveri status
git status

# Dodaj sve fajlove
git add .

# Commit sa opisnom porukom
git commit -m "feat: initialize Python automation portfolio with web scraper MVP

- Add comprehensive README for portfolio
- Implement quotes scraper (requests + BeautifulSoup + pandas)
- Add config.py for easy customization
- Include detailed project README with usage examples
- Setup .gitignore for Python projects
- Create folder structure (projects/, learning/, docs/)

Day 01 complete: scraper tested and working!"

# Vidi commit
git log --oneline -1
````

**Zadatak 4.3: GitHub Push (15min)**

```bash
# Kreiraj novi repo na GitHub (via web ili gh CLI)
# Zatim:

git remote add origin https://github.com/jole-pavlovic-dev/python-automation-portfolio.git
git branch -M main
git push -u origin main
```

---

### BLOK 5: Reflection + Plan Tomorrow (19:00-19:30) - 30min

**Zadatak 5.1: Napravi DAY_01_SUMMARY.md**

```markdown
# Day 01 Summary

**Date:** 13. decembar 2025.

## âœ… Completed

-   [x] Repo kreiran i inicijalizovan
-   [x] Python environment setup (venv)
-   [x] Dependencies installed (requests, beautifulsoup4, pandas)
-   [x] Python syntax refresh (functions, loops, dicts, files)
-   [x] Requests library testing
-   [x] BeautifulSoup testing
-   [x] **PRVI PRAVI SCRAPER NAPISAN I TESTIRAN!** ğŸ‰
-   [x] CSV export funkcioniÅ¡e
-   [x] README dokumentacija
-   [x] Git commit + push na GitHub

## ğŸ“Š Stats

-   Lines of code written: ~300+
-   Files created: 8
-   Quotes scraped: 20
-   Commits: 1 (solid!)

## ğŸ§  What I Learned

-   Kako funkcioniÅ¡e requests.get() i response.status_code
-   BeautifulSoup parsing sa find() i find_all()
-   CSS selectors za preciznije selektovanje
-   Pandas DataFrame.to_csv() za export
-   Best practice: config.py za settings
-   Error handling sa try/except blokovima

## ğŸ’ª Challenges

-   [NapiÅ¡i Å¡ta ti je bilo teÅ¡ko]
-   [Gde si zapeo i kako si reÅ¡io]

## ğŸ¯ Tomorrow (Day 02)

1. Dodaj command-line arguments (argparse)
2. Implementiraj error logging u fajl
3. Refaktoruj kod - napravi scraper class
4. Unit tests sa pytest (osnovni)
5. Selenium setup za JS-heavy sites (optional)

## ğŸ—£ï¸ Notes

-   [Tvoje opservacije]
-   [Å ta ti se sviÄ‘a u Python-u]
-   [Plan za sledeÄ‡u nedelju]

---

**Energy Level:** [1-10]
**Confidence Level:** [1-10]
**Ready for Day 02:** [YES/NO]
```

---

## ğŸ‰ ÄŒESTITAM

Ako si doÅ¡ao ovde, zavrÅ¡io si Day 01! Evo Å¡ta si postigao:

âœ… **Repo kreiran** - python-automation-portfolio na GitHub-u
âœ… **Prvi projekat** - Web Scraper Tool (funkcioniÅ¡e!)
âœ… **Python sintaksa** - osveÅ¾ena i testirana
âœ… **Real code written** - 300+ linija production-ready koda
âœ… **Git workflow** - commit + push mastered

---

## ğŸš€ Å ta Dalje?

**Sutra (Day 02):**

-   Dodaj CLI argumente
-   Implementiraj logging
-   Refaktoruj u class-based strukturu
-   Osnovno unit testing

**SledeÄ‡ih 7 dana:**

-   Week 1: Web Scraper projekt complete (advanced features)
-   Dec 21: Kreni na Project 2 - Excel Automation

**3-Month Goal:**

-   4 portfolio projekta
-   Freelance profiles live (Upwork/Fiverr)
-   Prvi klijent do kraja januara 2026

---

## ğŸ’¡ Pro Tips

1. **Commit Äesto!** Svaki feature = novi commit
2. **Testiraj u malim koracima** - nemoj pisati 200 linija pa onda testirati
3. **ÄŒitaj error poruke** - Python errors su super informativne!
4. **Pitaj AI** - ako zaglaviÅ¡ 15+ minuta, pitaj za pomoÄ‡
5. **Pravi pauze** - svaka 2h stanke 10-15min

---

**See you tomorrow at 09:00! ğŸš€**

**ALL OR NOTHING!** ğŸ’ª
