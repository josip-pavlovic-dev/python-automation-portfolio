# üï∑Ô∏è WEB SCRAPER v1 ‚Äî Dan 8: Setup & Foundation

**Datum:** 26. Decembar 2025
**Status:** üî¥ AKTIVNO (Dan 8/14)
**Trajanje:** 8 sati
**Prethodni Dan:** ‚úÖ Dan 6-7 (Pathlib + Testing + Error Handling Complete)

---

## üéØ ≈†ta Radi≈° Danas (Dan 8)?

Danas kreirate **foundation** za Web Scraper projekat. Ovo je **SETUP DAN** ‚Äî ne scrape-ovanja jo≈°, veƒá infrastrukture!

### Oƒçekivani Rezultat na Kraju Dana

```
projects/01-web-scraper/
‚îú‚îÄ‚îÄ venv/                          # Virtual environment
‚îú‚îÄ‚îÄ config.py                       # Konfiguracija
‚îú‚îÄ‚îÄ scraper.py                      # Main scraper (prazna struktura)
‚îú‚îÄ‚îÄ requirements.txt                # Dependencies
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                 # Pytest fixtures
‚îÇ   ‚îî‚îÄ‚îÄ test_scraper_basics.py      # Prvi testovi
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ app.log                      # Log fajl
‚îú‚îÄ‚îÄ output/
‚îÇ   ‚îî‚îÄ‚îÄ sample.csv                   # Output primer
‚îî‚îÄ‚îÄ README.md                        # Project dokumentacija

‚úÖ Sve je verzionovano u Git
‚úÖ Svi testovi prolaze
‚úÖ Struktura je proizvodna
‚úÖ Logovanje je konfigurisano
```

---

## üìö ≈†ta ƒÜe≈° Nauƒçiti?

### 1. **HTTP Requests osnove** (1h)

```python
import requests

response = requests.get("https://example.com")
print(response.status_code)  # 200
print(response.text)         # HTML sadr≈æaj
```

### 2. **HTML Parsing sa BeautifulSoup** (1h)

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(html, "html.parser")
titles = soup.select("h1")  # CSS selektori
```

### 3. **Project Setup i Struktura** (2h)

```
≈†ta kreiramo:
- config.py sa settings (URL, headers, timeout)
- scraper.py sa basic funkcijama
- tests/ sa test fixtures
- Logging konfiguracija
```

### 4. **Error Handling pri Scraping-u** (2h)

```python
try:
    response = requests.get(url, timeout=5)
    response.raise_for_status()  # Baci gre≈°ku ako je 404/500
except requests.RequestException as e:
    logger.error(f"Request failed: {e}")
```

### 5. **REPL Praksa + Testiranje** (2h)

```bash
# Pokrenite REPL i eksperimentirajte
python3
>>> from scraper import fetch_page
>>> html = fetch_page("https://example.com")
>>> print(len(html))
```

---

## üó∫Ô∏è Struktura Uƒçenja Dana 8

### FAZA 1 (1.5h) ‚Äî Requests + BeautifulSoup Osnove

-   ‚úÖ Instalacija libraryja
-   ‚úÖ GET request-i
-   ‚úÖ HTML parsing sa CSS selektorima
-   ‚úÖ REPL praksa

### FAZA 2 (1.5h) ‚Äî Project Setup

-   ‚úÖ Kreiranje `projects/01-web-scraper` strukture
-   ‚úÖ config.py sa settings-ima
-   ‚úÖ requirements.txt sa zavisnostima
-   ‚úÖ Git inicijalizacija

### FAZA 3 (2h) ‚Äî Scraper Osnova

-   ‚úÖ `scraper.py` sa `fetch_page(url)` funkcijom
-   ‚úÖ User-Agent headers
-   ‚úÖ Error handling
-   ‚úÖ Logging

### FAZA 4 (2h) ‚Äî Testing Setup

-   ‚úÖ `conftest.py` sa fixtures-ima
-   ‚úÖ Mock test stranice
-   ‚úÖ Prvi `test_scraper_basics.py` testovi
-   ‚úÖ Coverage proveravanja

---

## üìñ Kako Koristi≈° Materijal?

### 1Ô∏è‚É£ **Zapoƒçni sa kickoff.md** (5 min)

-   Brz pregled timeline-a
-   ≈†ta treb≈° ako zaglavim

### 2Ô∏è‚É£ **ƒåitaj cheatsheet.md** (30 min)

-   Requests dokumentacija
-   BeautifulSoup CSS selektori
-   Error handling patterns

### 3Ô∏è‚É£ **Prati web_scraper_setup_guide.md** (6h)

-   **GLAVNI MATERIJAL** ‚Äî korak po korak ve≈æbe
-   REPL primeri
-   ƒåesto ponovljeni kodovi

### 4Ô∏è‚É£ **Koristi tasks.md** (Tokom dana)

-   Checklist ≈°ta treba da zavr≈°i≈°
-   Checkpoint-i

### 5Ô∏è‚É£ **Ako Zaglavim ‚Üí chatlog.md**

-   ƒåesta pitanja
-   Re≈°enja za ƒçeste gre≈°ke

---

## üîó Povezivanja sa Prethodnim Danima

| Dan         | ≈†to Si Nauƒçio                 | Kako Se Koristi Danas            |
| ----------- | ----------------------------- | -------------------------------- |
| **Dan 5**   | Type Annotations (TypedDict)  | `class ScrapedItem(TypedDict):`  |
| **Dan 6**   | Pathlib (`Path`, `mkdir`)     | `Path("output") / "data.csv"`    |
| **Dan 7**   | Pytest (`tmp_path`, `caplog`) | `test_scraper_basics.py` testovi |
| **Dan 6-7** | Error Handling                | Try/except u scraper-u           |
| **Dan 1-4** | CSV + Logging + CLI           | Saƒçuvaj u CSV, loguj, CLI args   |

---

## ‚ö° Quick Reference

### Instalacija Dependencies

```bash
cd projects/01-web-scraper
python -m venv venv
source venv/bin/activate
pip install requests beautifulsoup4 pytest pytest-cov
pip freeze > requirements.txt
```

### Testiranje

```bash
pytest tests/test_scraper_basics.py -v
pytest --cov=scraper --cov-report=term-missing
```

### REPL Praksa

```bash
source venv/bin/activate
python3

# Isprobaj requests
>>> import requests
>>> r = requests.get("https://httpbin.org/html")
>>> print(r.status_code)
200

# Isprobaj BeautifulSoup
>>> from bs4 import BeautifulSoup
>>> soup = BeautifulSoup(r.text, "html.parser")
>>> print(soup.prettify()[:200])
```

---

## üéØ Minimalni Zahtev za Dan 8

**MINIMUM (da bih mogao nastaviti na Dan 9):**

```
1. ‚úÖ Instalisan requests i beautifulsoup4
2. ‚úÖ Kreiran projects/01-web-scraper sa strukturom
3. ‚úÖ config.py sa settings-ima (URL, headers)
4. ‚úÖ scraper.py sa fetch_page(url) funkcijom
5. ‚úÖ test_scraper_basics.py sa 3+ testova
6. ‚úÖ Sve testove prolaze (pytest -v)
7. ‚úÖ Coverage >70%
8. ‚úÖ Git commit sa porukom "Day 8: Web Scraper Setup"
```

---

## üí° Pro Tips za Dan 8

1. **Koristi `httpbin.org` za testiranje** ‚Äî fake endpoint za ve≈æbu
2. **Dodaj User-Agent header** ‚Äî neki serveri odbijaju bot zahteve
3. **Postavi timeout** ‚Äî izbegni da se script zamrzne
4. **Loguj sve zahteve** ‚Äî debug je lak≈°i sa logovima
5. **Testiranje sa mock-om** ‚Äî ne testira pravi server svaki put
6. **ƒåitaj requests dokumentaciju** ‚Äî samo 5 minuta ali je korisno

---

## üìû Support Struktura

### Ako Se Zaglavi≈° na Requests

```bash
# Proveri status koda
>>> response.status_code
200  # OK
404  # Not found
500  # Server error

# Proveri headers
>>> response.headers
{'content-type': 'text/html; charset=utf-8', ...}

# Proveri sadr≈æaj
>>> response.text[:100]
```

### Ako Se Zaglavi≈° na BeautifulSoup

```python
# Sve ≈°to je u <div class="title">
div = soup.select_one("div.title")

# Sve <a> tagove
links = soup.select("a")

# Tekst iz elementa
text = soup.select_one("h1").get_text(strip=True)
```

### Ako Se Zaglavi≈° na Pytest

```bash
# Pokreni samo jedan test
pytest tests/test_scraper_basics.py::test_fetch_page_returns_string -v

# Vidi print output
pytest -s tests/test_scraper_basics.py

# Vidi coverage
pytest --cov=scraper tests/
```

---

## üìö Reference Materijali

**Local Files:**

-   [cheatsheet.md](./cheatsheet.md) ‚Äî Requests + BeautifulSoup reference
-   [web_scraper_setup_guide.md](./web_scraper_setup_guide.md) ‚Äî **GLAVNI MATERIJAL**
-   [tasks.md](./tasks.md) ‚Äî Checklist
-   [chatlog.md](./chatlog.md) ‚Äî Q&A

**Official Docs:**

-   [Requests Docs](https://docs.python-requests.org/)
-   [BeautifulSoup Docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
-   [Pytest Docs](https://docs.pytest.org/)

**Prethodni Materijali:**

-   `learning/2025-12-22_pathlib_advanced/cheatsheet.md` ‚Äî Pathlib refresh
-   `scratch/docs/cheatsheet_error_handling.md` ‚Äî Error handling patterns
-   `scratch/docs/cheatsheet_pytest_testing.md` ‚Äî Pytest fixtures

---

## üöÄ ≈†ta Dolazi Dan 9?

**Dan 9-10:** Web Scraper Implementation

```python
# To ƒáe biti tvoj kod na Dan 9:
class Product(TypedDict):
    title: str
    price: float
    url: str

def scrape_products(url: str) -> list[Product]:
    """Scrape proizvode sa CSS selektorima"""
    html = fetch_page(url)
    items = parse_products(html)
    return items
```

---

## ‚úÖ Checklist Pre Nego Poƒçne≈°

Proveri da li ima≈° sve od Dana 6-7:

-   [ ] Razumem `Path` operacije (Pathlib)
-   [ ] Mogu da napravim jednostavan test sa `tmp_path`
-   [ ] Znam kako koristiti `pytest.raises()` za gre≈°ke
-   [ ] Razumem try/except/else/finally
-   [ ] Znam kako da napravim custom exception
-   [ ] Mogu da koristim `logging` modul

Ako je sve ‚úÖ, spreman si za Dan 8! üöÄ

---

**Status: READY TO START**

**Start Time: Kada god ≈æeli≈°**

**Duration: 8 hours**

**Next: [kickoff.md](./kickoff.md)**

---
